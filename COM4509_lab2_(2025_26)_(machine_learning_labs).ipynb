{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyamadhavan/getting_started/blob/main/COM4509_lab2_(2025_26)_(machine_learning_labs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuCRno8ChJZc"
      },
      "source": [
        "# 1. Introduction\n",
        "<font color='#777'>Based on a notebook by Mauricio A Álvarez</font>\n",
        "\n",
        "In this lab we will be using using [scikit-learn](https://scikit-learn.org/stable/) (a popular machine learning library) to predict bike rentals.\n",
        "\n",
        "The purpose of this lab is to:\n",
        " - See how one might load and explore/visualise a dataset\n",
        " - See an example of an invalid train/test split that introduces spurious correlations\n",
        " - Use an sklearn model to try and make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOg-NMP5oRR6"
      },
      "source": [
        "# 2. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIBmjr0LhJZi"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "\n",
        "urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv', './SeoulBikeData.csv')\n",
        "bike_sharing_data = pd.read_csv('SeoulBikeData.csv', encoding= 'unicode_escape')\n",
        "\n",
        "bike_sharing_data = bike_sharing_data.drop('Date', axis=1) #we don't use this column so we remove it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iofPOVHshJZl"
      },
      "source": [
        "We can get a description of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U_45gu7hJZm"
      },
      "outputs": [],
      "source": [
        "bike_sharing_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_u4ULNMhJZm"
      },
      "source": [
        "We can see some of the rows in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZqjm32phJZn"
      },
      "outputs": [],
      "source": [
        "bike_sharing_data.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9URjKkOxhJZo"
      },
      "source": [
        "- The target ('dependent') variable, that we're interested in (that we call $y$) corresponds to the Rented Bike Count variable.\n",
        "- The feature/attribute/'independent' variables are made of the next twelve columns. So *hour* is $x_1$, *Temperature* is $x_2$...etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvoX6P-hJZp"
      },
      "source": [
        "We follow some of the steps in the ML checklist we used in the lecture, including data exploration, data preprocessing, and optimising the model parameters.\n",
        "\n",
        "- Remember: test data that we later use for assessing the generalisation performance should be set aside when we first get the data.\n",
        "\n",
        "- Any data preprocessing that you do should mostly be done just on the training data. Separating the dataset into training and test before any preprocessing has happened, helps us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing. Here though we'll first convert the integer columns to floats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgm7pTmHhJZq"
      },
      "outputs": [],
      "source": [
        "bike_sharing_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVPS1jgehJZq"
      },
      "source": [
        "Several algorithms that we will use assume the inputs to be type 'float' instead of 'int', so we transform those variables in the dataset from int64 to float64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zibLQdOnhJZr"
      },
      "outputs": [],
      "source": [
        "for col in ['Rented Bike Count', 'Hour', 'Humidity(%)', 'Visibility (10m)']:\n",
        "    bike_sharing_data[col] = bike_sharing_data[col].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHZ6df4UhJZr"
      },
      "outputs": [],
      "source": [
        "bike_sharing_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQVF_3oGhJZr"
      },
      "source": [
        "The dataset has a few thousand observations. We will use 85% of the data for training and 15% for testing. The `train_test_split` function in scikit-learn allows to easily get these partitions.\n",
        "\n",
        "- By specifying a value for `random_state`, we are making sure that every time we run this instruction, the train and test set will have the exact same instances. `random_state` \"controls the shuffling applied to the data before applying the split\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDiJ_SFJhJZs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "bs_train_set, bs_test_set = train_test_split(bike_sharing_data, test_size=0.15, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlEj5g9thJZs"
      },
      "source": [
        "- The train and test sets are chosen randomly from all the available data.\n",
        "\n",
        "### Question 1\n",
        "\n",
        "1. Discuss whether this split is a good choice: Will it over-inflate our accuracy estimate?\n",
        "2. Does it depend on which classifier we use?\n",
        "3. What does it mean for generalisation?\n",
        "4. What steps have we skipped from end-to-end ML?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpRpxbXdhJZt"
      },
      "source": [
        "---\n",
        "\n",
        "Answer here\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIpks09phJZt"
      },
      "source": [
        "# 3. The train/test split and introducing spurious correlations\n",
        "\n",
        "The key issue when answering the above questions is to think about spurious correlations that might inflate your accuracy.\n",
        "\n",
        "Let's consider a simple, synthetic/toy example dataset.\n",
        "\n",
        "We want to predict if we will make a profit from the bike hire scheme. We have a column for if a given hour is profitable (i.e. enough bikes are in use to turn a profit). We also have a column for the number of wind gusts that hour (we think that this can help predict our profits!).\n",
        "\n",
        "We have four observations for each day (0-9). Maybe we could imagine that they are from e.g. 9am, 12noon, 3pm and 6pm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBHNP6OChJZu"
      },
      "outputs": [],
      "source": [
        "#windy  bike hires\n",
        "import numpy as np\n",
        "\n",
        "example = np.array([[0,154],[0,153], [0,152], [0,153], [0,132],[0,133], [0,131], [0,131], [1,74],[1,72], [1,71], [1,73], [0,53],[0,52], [0,53], [0,55], [0,121],[0,123], [0,122],[0,120], [1,11],[1,14], [1,12], [1,15], [1,2],[1,3],[1,5],[1,4], [1,142],[1,143],[1,140],[1,139], [0,45],[0,46],[0,44],[0,43], [1,89],[1,88],[1,90],[1,85]])\n",
        "exampleday = np.array([0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8,9,9,9,9])\n",
        "df = pd.DataFrame(np.c_[exampleday,example],columns=['day','profit','gusts'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baiA4sThhJZu"
      },
      "source": [
        "Question: Looking at the data, do you think that the number of gusts can *really* help predict the profit?\n",
        "\n",
        "Let's find out. We split the data 70:30, randomly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPle9Pe2hJZu"
      },
      "outputs": [],
      "source": [
        "train_example, test_example = train_test_split(df, test_size=0.3, random_state=2)\n",
        "print(\"TRAIN:\")\n",
        "print(train_example)\n",
        "print(\"TEST:\")\n",
        "print(test_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss2GfPr_hJZv"
      },
      "source": [
        "We train a nearest neighbour classifier on the training data, and predict on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx-PopndhJZv"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "neigh.fit(train_example[['gusts']],train_example['profit'])\n",
        "print(neigh.predict(test_example[['gusts']])==test_example['profit'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWER3rqChJZv"
      },
      "source": [
        "Great... it classified them all correctly.\n",
        "\n",
        "The problem is that the number of gusts between two hours on the same day is strongly correlated. But an hour might be profitable for many other reasons. So it is likely that this success is due to having correlations in the number of gusts between hours on the same day.\n",
        "\n",
        "This is call [leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)#Training_example_leakage).\n",
        "\n",
        "To mitigate this, we need to be more careful about how we split our data. As an example, we can use `GroupShuffleSplit` to do this instead which allows us to pass a `groups` parameter telling it which rows are in the same group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD0PbNgrhJZv"
      },
      "source": [
        "Here we are splitting the data using this method, note where we pass `groups = df['day']` to say how we want the data to be grouped when splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWloWtyRhJZw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=2)\n",
        "gss.get_n_splits()\n",
        "train_idx, test_idx = next(gss.split(df, groups = df['day']))\n",
        "train_group_example = df.iloc[train_idx]\n",
        "test_group_example = df.iloc[test_idx]\n",
        "print(\"TRAIN:\")\n",
        "print(train_group_example)\n",
        "print(\"TEST:\")\n",
        "print(test_group_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ztN_zNhJZw"
      },
      "source": [
        "Notice that different hours from the same day are in the same set.\n",
        "\n",
        "Let's run the classifier again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2lIIXLThJZw"
      },
      "outputs": [],
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "neigh.fit(train_group_example[['gusts']],train_group_example['profit'])\n",
        "print(neigh.predict(test_group_example[['gusts']])==test_group_example['profit'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D77b9K9AhJZw"
      },
      "source": [
        "It's got 4 out of 6 of the predictions **wrong**! Maybe the number of wind gusts isn't useful for predicting profit afterall?\n",
        "\n",
        "<mark>Take home message: The correlations in the data can lead to artificially inflated accuracies. Think carefully about how you split your data!</mark>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWKZR1hRhJZw"
      },
      "source": [
        "# 4. Visualising and Exploring the dataset\n",
        "\n",
        "Back to the lab and the bike hire rate prediction problem....\n",
        "\n",
        "Note: we are going to continue using the `train_test_split` approach - but notice that in this data we will definitely be wrongly inflating accuracy, as neighbouring hours of bike-hire activity are probably correlated in such a way that the analysis is wrong. Ideally we should use a similar approach to above to split the data correctly.\n",
        "\n",
        "## Histograms\n",
        "\n",
        "Let us first look at histograms for each of the continuous attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D_xQ_bthJZx"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "bs_train_set.hist(bins=50, figsize=(20,15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaMv7bgrhJZx"
      },
      "source": [
        "Some observations from the histograms are:\n",
        "\n",
        "1. The values for the variables Rainfall, Snowfall, Solar Radition and Visibility are concentrated at one of the ends of the plots. This is an indication that several instances might contain outliers. One can consider removing these outliers from the data or binning the data into a few discrete values.\n",
        "\n",
        "2. Both the Rented Bike Count and the Wind Speed are skewed (https://en.wikipedia.org/wiki/Skewness). Some ML algorithms find it harder to detect patterns for this type of distribution. One might consider transforming these features using $\\log(x)$ or $\\sqrt{x}$ so that they look more like a bell-shaped distribution.\n",
        "\n",
        "## Question 2\n",
        "\n",
        "1. Compute the mean and the median for the variables Rented Bike Count and Wind Speed and verify that the mean is to the right of the median.\n",
        "2. How would the histograms for Rented Bike Count and the Wind Speed look like if we transform the values using $\\sqrt{x}$?\n",
        "3. Would it be possible to use $\\log{x}$ instead of $\\sqrt{x}$? If not, what would you do to the variable to be able to use it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnKwSiOhhJZx"
      },
      "outputs": [],
      "source": [
        "#Provide your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv5_FTD-hJZx"
      },
      "source": [
        "## Scatter plots\n",
        "\n",
        "The Scatter plot is a tool we can use to explore dependencies between the different variables. It contains plots of each variable against each other in the dataset. If there are many variables in the feature vector, including all scatter plots might not be convenient to visualise. Let us look at the scatter plot for the target variable and four of the attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C32JBYnZhJZx",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "attributes = ['Rented Bike Count', 'Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)']\n",
        "figscat = scatter_matrix(bs_train_set[attributes], figsize=(20, 15),alpha=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFAHgLtPhJZy"
      },
      "source": [
        "The variables Hour and Temperature seem correlated with Rented Bike Count. The relationship between Humidity and Wind Speed with Rented Bike Count looks less clear though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QtK8PGLhJZz"
      },
      "source": [
        "# 5. Prepare the data\n",
        "\n",
        "We will now prepare the data so that it is suitable for the machine learning models. We do two things in this notebook,\n",
        "\n",
        "1. [OneHotEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=onehotencoder#sklearn.preprocessing.OneHotEncoder) allows to transform a categorical variable to a one-hot encoding representation.\n",
        "\n",
        "2. [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler) performs feature scaling by standardisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGNA5fcGhJZz"
      },
      "source": [
        "`OneHotEncoder()` and `StandardScaler()` are examples of [data transformations](https://scikit-learn.org/stable/data_transforms.html). In scikit-learn these are referred to as *transformers* and they map the data from one format to another. In a programming context, transformers are classes. They come with the following methods:\n",
        "\n",
        "- `fit` that is used to learn the  transformation from data.\n",
        "- `transform` that is used to transform the data once the transformer has been fitted.   \n",
        "- `fit_transform` that applies first `fit` and then `transform` to the data.\n",
        "\n",
        "Typically, we use either `fit` or `fit_transform` for the training data and `transform` for the validation or test data.\n",
        "\n",
        "Since the one-hot-encoding and standardisation transformations are often used, rather than code such function from scratch we make use of the [ColumnTransformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer), an estimator available in scikit-learn that allows to group different transformations into a single method. `ColumnTransformer` is an example of an *estimator* in scikit-learn. An estimator is an object that provides predictions for new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OjruaJxhJZ0"
      },
      "source": [
        "To apply the transformation we need a list of the categorical attributes and a list of the numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37bKJtPjhJZ0"
      },
      "outputs": [],
      "source": [
        "attributes_cat = ['Seasons', 'Holiday', 'Functioning Day']\n",
        "attributes_num = ['Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', \\\n",
        "                  'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkIzLtrYhJZ1"
      },
      "source": [
        "We now import `OneHotEncoder`, `StandardScaler` and `ColumnTransformer` and create the actual transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4kBYhhKhJZ1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "full_transform = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), attributes_num),\n",
        "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dKLbcvAhJZ1"
      },
      "source": [
        "Before applying the full transformation, we separate the target feature from the attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNlyohtHhJZ2"
      },
      "outputs": [],
      "source": [
        "bs_train_set_attributes = bs_train_set.drop('Rented Bike Count', axis=1)\n",
        "bs_train_set_labels = bs_train_set['Rented Bike Count']\n",
        "\n",
        "bs_test_set_attributes = bs_test_set.drop('Rented Bike Count', axis=1)\n",
        "bs_test_set_labels = bs_test_set['Rented Bike Count']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPto--JJhJZ2"
      },
      "source": [
        "We can now apply the full transformation to the training/test data using `fit_transform`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAje_yRVhJZ3"
      },
      "outputs": [],
      "source": [
        "bs_train_set_attributes_prepared = full_transform.fit_transform(bs_train_set_attributes)\n",
        "bs_test_set_attributes_prepared = full_transform.fit_transform(bs_test_set_attributes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thALlPUmfuka"
      },
      "source": [
        "## 6. Optimising Model Parameters\n",
        "\n",
        "We don't want to train yet (and test on the held-out test set), as we don't know what a good value of k might be. We should not use test data for determining k, but instead we could split our training data into training and validation. Typically one does this using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1PLrZl7zVhu"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knn = KNeighborsRegressor()\n",
        "#knn.fit(bs_train_set_attributes_prepared, bs_train_set_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JAA1gZBntv4"
      },
      "source": [
        "To find the names of possible parameters we need to learn..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq3o_Lf_npkX"
      },
      "outputs": [],
      "source": [
        "knn.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52yGBMdigC5e"
      },
      "source": [
        "Let's just optimise the number of neighbours for now,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwtWVOBfn2-Q"
      },
      "outputs": [],
      "source": [
        "param_grid = [  {'n_neighbors': list(range(1,20))} ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjIWX9ypgKDr"
      },
      "source": [
        "We will use 5-fold cross-validation, and we will assess accuracy with the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LemoxvJxpqDS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='neg_root_mean_squared_error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmGqJoy6pzLb"
      },
      "outputs": [],
      "source": [
        "grid_search.fit(bs_train_set_attributes_prepared, bs_train_set_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6PwEBBWgT6I"
      },
      "source": [
        "The best value of k to use (the number of neighbours to include) is,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60aXkbiT0vxL"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXib7qPkgYXN"
      },
      "source": [
        "We can set our kNN model to k=5, and train it on the full training set,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fyl7j0D79Aj4"
      },
      "outputs": [],
      "source": [
        "knn.set_params(**grid_search.best_params_)\n",
        "knn.fit(bs_train_set_attributes_prepared,bs_train_set_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz41ZfP9hJZ7"
      },
      "source": [
        "We can now compute the RMSE obtained with this predictive model. We can use the [scikit-learn routine for computing the mean squared error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) and then compute the square root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KftM1gmkiHpU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import root_mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "predictions = knn.predict(bs_train_set_attributes_prepared)\n",
        "rmse_train = root_mean_squared_error(predictions, bs_train_set_labels)\n",
        "print(\"RMSE error on training set %0.1f\" % rmse_train)\n",
        "\n",
        "predictions = knn.predict(bs_test_set_attributes_prepared)\n",
        "rmse_test = root_mean_squared_error(predictions, bs_test_set_labels)\n",
        "print(\"RMSE error on test set %0.1f\" % rmse_test)\n",
        "\n",
        "print(\"For context the average number rented each hour is about %d, with standard deviation %0.1f.\" % (bike_sharing_data['Rented Bike Count'].mean(),bike_sharing_data['Rented Bike Count'].std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7hb9A5XhJZ9"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "1. This data is being normalised using StandardScaler, try replacing the pipeline instruction line with,\n",
        "\n",
        "<pre>full_transform = ColumnTransformer([\n",
        "    (\"num\", \"passthrough\", attributes_num),\n",
        "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
        "])</pre>\n",
        "\n",
        "See what effect this has on the RMSE. Is kNN particularly susceptible to the quality of the scaling?\n",
        "2. Does replacing it with `MinMaxScaler` help?\n",
        "3. Should we be using the results on the test data to be improving our model?\n",
        "4. It is likely that the output changes slowly with the inputs, try using LinearRegression() instead.\n",
        "5. If you have time try other ways to improve the test accuracy -- again, remember we should really be making trying to improve validation set accuracy, rather than the held-out test set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQdZnI5tpVac"
      },
      "outputs": [],
      "source": [
        "# answers here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}